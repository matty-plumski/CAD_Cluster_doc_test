
<!-- Version: 0.9.4 (build #dcaa3875fc7d7a0480bd0d689c7e32bfbdead443) | Tue Mar 21 2017 2:48 -->
<!doctype html>
<html lang="">
<head>

    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title></title>
    <link rel="icon" href="images/favicon.ico" type="image/x-icon">
      <!-- Fav icons compatible with all major browsers and devices -->
  <link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16.png">
  <link rel="manifest" href="../site.webmanifest">
  <link rel="mask-icon" href="../images/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#00aba9">
  <meta name="theme-color" content="#ffffff">


  <!-- toolkit styles -->
  <link rel="stylesheet" href="https://static.victoria.ac.nz/toolkit.min.css">
  <!-- /toolkit styles -->

</head>


<body>


<!-- If 'fabricator=true' is present in page's annotation. -->

<!-- If 'fabricator=false' is present or attribute is not present at all in the annotation. -->

<div class="global" id="global-nav">

    <div class="menu-toggle horisontal-links">
        <a href="javascript:void(0);"><div class="logo-mini">&nbsp;</div></a>
        <!-- <a href="javascript:void(0);" class="icon-2x icon-menu js-toggle-global-nav align-right"></a> -->
        <!-- <a href="javascript:void(0);" class="lines-button x js-toggle-global-nav align-right" role="button" aria-label="Toggle Navigation"><span class="lines"></span></a> -->
        <a href="javascript:void(0);" class="js-toggle-global-nav" role="button" aria-label="Toggle Navigation">

    			<span class="tcon tcon-menu--xcross" aria-label="toggle menu" style="float:right;">
    			  <span class="tcon-menu__lines" aria-hidden="true"></span>
    			  <span class="tcon-visuallyhidden">toggle menu</span>
    			</span>

        </a>
        <!-- <div class="mobile logo-mini"></div> -->
    </div>

    <div class="menu">
        <nav>
            <div class="search">
                <form method="get" action="http://victoria.ac.nz/search">
                    <div role="search">
                        <a href="javascript:void(0);" class="js-toggle-global-search hide-on-mobile">
    						<span class="tcon tcon-search--xcross " aria-label="toggle search">
    						  <span class="tcon-search__item" aria-hidden="true"></span>
    						  <span class="tcon-visuallyhidden">toggle search</span>
    						</span>
                        </a>
                        <!-- <a href="#" class="icon-search js-toggle-global-search hide-on-mobile"></a> -->
                        <label class="input-wrapper -icon-search" id="global-search">

                            <input type="text" autocomplete="off" name="q" placeholder="Search Victoria for…" required>
                            <input type="submit" value="Go">

                        </label>
                    </div>
                </form>

                <div class="horisontal-links align-center hide-on-desktop">
                    <a href="#"><span class="icon-home"></span>Home</a>
                    <a href="#"><span class="icon-phone"></span>Contact</a>
                </div>
            </div>

            <div id="menu">
                <a href="#" class="home">
                    <span class="icon-globe hanging-icon"></span>
                    <span class="hide-on-desktop">CAD Research Cluster Documentation</span>
                </a>
                <a href="index.html">Cluster Description</a>
                <a href="accessing-the-cluster.html">Acessing the Cluster</a>
                <a href="basic-commands.html">Basic Commands</a>
                <a href="running-jobs.html">Running Jobs</a>
                <a href="pages/parallel-processing.html">Parallel Processing</a>
                <a href="managing-jobs.html">Managing Jobs</a>
                <a href="using-containers.html">Using Containers</a>
                <a href="examples.html">Examples</a>

            </div>
        </nav>
        </div> 
</div>
<!-- Default fallback for header when not specified in the template. -->
<header role="banner" class="site-header">
    <div class="block centraliser">

        <!-- Logotype -->
        <div class="logo">
            <a href="/" title="Victoria University of Wellington homepage">
                <picture>
                	<source media="(max-width: 43em)" srcset="images/logo-black-mini.svg" height="1">
                    <source media="(max-width: 68em)" srcset="images/logo-white-portrait.svg">
                    <img src="images/logo-white-full.svg" alt="Victoria University of Wellington - Te Whare Wānanga o te Ūpoko o te Ika a Māui">
                </picture>
            </a>
        </div>



        <!-- Introductory block -->
        <div class="site-intro" id="myHeader">

            <a href="#" title="homepage">
                <h1>
                  CAD Research Cluster Documentation
                    <small lang="mi"> The CAD Cluster is a Slurm-based high-performance computing cluster at VUW </small>
                </h1>
            </a>
        </div>

    </div>
</header>


 <!-- 'Go up' button -->
  <a href="javascript:;" id="btn-up" title="Go to the top of the page" class="btn-floating sticky bottom" tabindex="-1"><span class="icon-arrow-up"></span></a>

  <!-- 'Edit page' button -->
  <a href="/_edit" id="btn-admin" style="display: none;" title="Edit the page in Squiz administration" class="btn-floating top" tabindex="-1"><span class="icon-edit"></span></a>



	

<!-- 2) Sidebar + Content only -->
<section class="layout centraliser block">
  
    <!-- Left 'submenu' column -->
    <div class="sidebar"> 
      
        <nav class="site" role="navigation">
            <a href="#" class="back hide-on-desktop">
              <span class="icon-back hanging-icon"></span>
              <div>

              </div>
            </a>
          
            <!-- <p>No Children</p> -->
            <ul>
              <!-- Parents in descending order -->
              <li><a href="http://www.victoria.ac.nz/fhss">Faculty's Home</a></li>
              <li><a href="http://www.victoria.ac.nz/fhss/about">About us</a></li>
              <li><a href="http://www.victoria.ac.nz/fhss/about/committees-boards">Committees and boards</a></li>
          
              <li>
          
                <!-- Previous page -->
                <!-- <a href="http://www.victoria.ac.nz/fhss/about/committees-boards">Committees and boards</a> -->
          
                <!-- Subpages -->
                <ul>
          
                  <li>
          
                    <!-- Parent page -->
                    <a href="index.html" class="selected">
                      Cluster Description
                    </a>
          
                    <!-- All the children -->
                    <ul>
                      <li>
                        <a href="accessing-the-cluster.html" >
                          Accessing the cluster
                        </a>
                      </li>
                      <li>
                        <a href="basic-commands.html" >
                          Basic commands
                        </a>
                      </li>
                      <li>
                        <a href="running-jobs.html">
                          Running jobs
                        </a>
                      </li>
                          <li >
                        <a href="parallel-processing.html">
                          Parallel processing
                        </a>
                      </li>
                      <li>
                        <a href="managing-jobs.html">
                          Managing jobs
                        </a>
                      </li>
                      <li>
                        <a href="using-containers.html">
                          Using containers
                        </a>
                      </li>
                      <li class="has-subpages">
                        <a href="examples.html">
                          Examples
                        </a>
                      </li>

                    </ul>
                  </li>
          
                </ul>
          
              </li>
            </ul>
          </nav>
          
    </div>

    <style>
        .grid figure {
          cursor: pointer;
        }
      </style>
  
    <!-- Center 'content' and right 'widget' columns -->
    <div class="content-panel">


<!-- Center 'content' and right 'widget' columns -->
<main>
<div class="block formatting">

<div class="homepage-base layout">
  <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1 id="cluster-description">Cluster Description</h1>

<p>The CADGrid HPC Cluster (hereafter referred to as: the cluster) is a Uni-wide computing
resource that uses the Slurm resource manager to schedule jobs and reserve
resources.  Similar to most modern compute clusters, the cluster requires you to
request CPU, Memory and Time for your job.  If you do not request these
resources, you will be given the minimal defaults, which may not be enough to
run your job.  The good news about resource reservations is that the resources
you request are guaranteed to be yours, the bad news is if you request too
little memory or time, your job may terminate prematurely and if you request too
few CPUs then your job may run slowly.</p>

<p>The cluster is made up of partitions.  A partition is a set of compute nodes
(servers) and each partition has its own configuration and hardware profile.
The partition on which you run your jobs will depend on the type of workflow or
job you intend to submit.</p>

<p>The cluster employs the module environment to allow researchers to customize
their environment with their required applications and languages.</p>

<p>The documentation contained in this wiki cover most of what you will need to
know to start running jobs, but if you need more help, we will be creating some
tutorials with step-by-step instructions for our most popular apps and
languages.</p>

<h1 id="accessing-the-cluster">Accessing the cluster</h1>

<p><em>Access is via SSH</em></p>

<ul>
  <li>Hostname: 10.60.49.210</li>
  <li>Port: 22</li>
  <li>Username: Your VUW username</li>
  <li>Password: Your cluster password</li>
</ul>

<p>Note: We recommend against saving your password within your SSH client, this is counter to security best-practice and will most likely cause issues in the future.</p>

<h4 id="ssh-clients">SSH Clients</h4>
<p><em>Mac OSX SSH Clients</em>
You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding.</p>

<ul>
  <li>Terminal.app is the default application for command-line interface
    <ul>
      <li>To login using the built-in Terminal.app on Mac, go to
        <ul>
          <li>Applications –&gt; Utilities –&gt; Terminal.app</li>
          <li>Or use Spotlight search (aka Command-Space)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://www.iterm2.com/">iTerm2</a> is a good replacement for the default Terminal app</li>
  <li><a href="https://www.xquartz.org/">XQuartz</a> is a Xforwarding application with its own terminal</li>
</ul>

<p>NOTE:  Once at the command prompt you can type the following to login (replace “username” with your VUW user):</p>

<p><code class="highlighter-rouge">ssh username@10.60.49.210</code></p>

<p><em>Windows SSH Clients</em></p>

<ul>
  <li>Free Clients:
    <ul>
      <li><a href="https://mobaxterm.mobatek.net/">MobaXterm</a> is a good option</li>
      <li><a href="https://www.putty.org/">PuTTy</a></li>
    </ul>
  </li>
  <li>There are also the XWin32 and SecureCRT clients, but these cost money to license.
    <ul>
      <li><a href="https://www.starnet.com/xwin32/">X-Win32</a></li>
      <li><a href="https://www.vandyke.com/products/securecrt/">SecureCRT</a></li>
    </ul>
  </li>
</ul>

<h1 id="basic-commands">Basic Commands</h1>
<h4 id="the-vuw-commands">The<em>vuw</em> Commands</h4>

<p>In an effort to make using the cluster just a bit easier, CAD staff have created commands to help you view useful information.  We call these the <em>vuw</em> commands.  This is because all the commands begin with the string <em>vuw</em>.  This makes it easier to see the commands available to you.  If, at a command prompt you type <em>vuw</em> followed immediately by two <em>TAB</em> keys you will see a list of available commands beginning with <em>vuw</em>.  Go ahead and type vuw-TAB-TAB to see for yourself.</p>

<p>The commands available as of this update are:</p>

<ul>
  <li><em>vuw-help</em> :            Prints this help information</li>
  <li><em>vuw-job-report</em> :      Provides some summary information about a job</li>
  <li><em>vuw-quota</em> :           Prints current storage quota and usage</li>
  <li><em>vuw-partitions</em> :      Prints a list of available partitions and the availability of compute nodes</li>
  <li><em>vuw-alljobs</em> :         Prints a list of all user jobs</li>
  <li><em>vuw-myjobs</em> :          Prints a list of your running or pending jobs</li>
  <li><em>vuw-job-history</em> :     Show jobs finished in last 48 hours</li>
</ul>

<h4 id="linux-commands">Linux Commands</h4>

<p>The cluster is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own):</p>

<p><strong>ls</strong> - This command lists the contents of the current directory</p>
<ul>
  <li><em>ls -l</em> This is the same command with a flag (-l) which lists the contents with more information, including access permissions</li>
  <li><em>ls -a</em> Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period)</li>
  <li><em>ls -la</em> Stringing flags together</li>
</ul>

<p><strong>cd</strong> - This will change your location to a different directory (folder)</p>
<ul>
  <li><em>cd projects/calctest_proj</em></li>
  <li>Typing <em>cd</em> with no arguments will take you back to your home directory</li>
</ul>

<p><strong>mv</strong> - This will move or rename a file</p>

<ul>
  <li><em>mv project1.txt project2.txt</em></li>
  <li><em>mv project2.txt projects/calctest_proj/</em></li>
</ul>

<p><strong>rm</strong> - This will delete a file</p>

<ul>
  <li><em>rm projects/calctest_proj/projects2.txt</em></li>
  <li><em>rm -r projects/calctest_proj/code</em>
The <em>-r</em> flag recursively removes files and directories</li>
</ul>

<p><strong>mkdir</strong> - This will create a new directory</p>
<ul>
  <li><em>mkdir /home/myusername/financial</em></li>
</ul>

<p>Other Commands you may use: <em>alias, awk, cat, export, for, grep, gzip, if, less, sed, tar, while</em></p>

<h4 id="learning-the-linux-shell">Learning the Linux Shell</h4>

<p>A good tutorial for using linux can be found here:
<a href="http://linuxcommand.org/lc3_learning_the_shell.php">Learning the linux shell</a></p>

<h1 id="preparing-your-environment">Preparing your environment</h1>

<p>The CAD Cluster has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Stata, etc).  We also keep older versions of software to ensure compatibility.</p>

<p>Because of this, the cluster developers use a tool called module to allow a user to load a specific version of an application, language or library and start using it for their work. The <em>module</em> command will show you what software is available to load, and will add the software to your environment for immediate use. To show all software available to load type the following:</p>

<p><code class="highlighter-rouge">module avail</code></p>

<p>You will see a long list of available modules to load, including a path, eg <em>lua/5.3.5</em>
However, instead of searching through a long list, if you know you want to use lua, you can find the path with the eo-module-find command:</p>

<p><code class="highlighter-rouge">module keyword lua</code></p>

<p>If you want to know more about a particular module you can use the whatis subcommand.  Some modules have this available, for instance:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  module whatis R/CRAN/3.5

  R/CRAN/3.5          : Adds the R library path to the pre-built CRAN modules
</code></pre></div></div>

<h4 id="adding-or-loading-software">Adding or loading software</h4>

<p>Once you have found the module path you can load the software:</p>

<p><code class="highlighter-rouge">module load lua/5.3.5</code></p>

<p>After the module loads you can type srun –pty lua at a prompt, or add it to the path of your lua script (the RC team recommends using /usr/bin/env instead of an absolute path).</p>

<p>Showing/listing the module environment modifications
You can discover what the module will load into your environment you can run module show, for example here is what gurobi adds:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module show R/3.5.1
--------------------------------------------------
   /home/software/vuwrc/modulefiles/R/3.5.1:
--------------------------------------------------

whatis("Adds the R language path to your environment ")
prepend_path("PATH","/home/software/apps/R/3.5.1/bin")
</code></pre></div></div>

<h4 id="listing-loaded-modules">Listing loaded modules</h4>

<p>To see what modules you have loaded into your environment you can run the command:</p>

<p><em>module list</em></p>

<p>By default you will have the config module loaded (please do not unload that module).  For example, here are the modules I have loaded in my environment when I wrote this section:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module list

Currently Loaded Modules:
  1) config   2) tassel/3   3) python/3.7.0   4) python/modules/3.7

</code></pre></div></div>

<h1 id="running-jobs">Running jobs</h1>

<h4 id="batch-jobs">Batch jobs</h4>

<p>To run a batch job (aka a job that runs unattended) you use the <em>sbatch</em> command.  A simple example would look something like this:</p>

<p><code class="highlighter-rouge">sbatch myjob.sh</code></p>

<p>In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a “batch submit script” could look something like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c">#!/bin/bash</span>
 <span class="c">#SBATCH --cpus-per-task=2</span>
 <span class="c">#SBATCH --mem-per-cpu=2G</span>
 <span class="c">#SBATCH --partition=main</span>
 <span class="c">#SBATCH --time=3-12:00</span>
 <span class="c">#SBATCH -o /home/username/project1.out</span>
 <span class="c">#SBATCH -e /home/username/project1.err</span>
 <span class="c">#SBATCH --mail-type=BEGIN,END,FAIL</span>
 <span class="c">#SBATCH --mail-user=me@email.com</span>

 module load python/3.6.3
 python3 project1.py

</code></pre></div></div>

<p>This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours.  We are requesting that this job be run on the main partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py.  Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err.  If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran <em>sbatch</em>.</p>

<p>NOTE:  We have this example script available to copy on the cluster, you can type the following to copy it to your home directory:</p>

<p><code class="highlighter-rouge">cp /home/software/vuwrc/examples/batch/myjob.sh ~/myjob.sh</code></p>

<p>The ~/ in front of the file is a short-cut to your home directory path.  You will want to edit this file accordingly.</p>

<h4 id="interactive-jobs">Interactive jobs</h4>

<p>One of the basic job submittal tools is the command srun</p>

<p>For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  module load R/3.5.1
  srun --pty --cpus-per-task=2 --mem=2G  --time=2-00:00 --partition=main R
</code></pre></div></div>

<p>So what does this all mean?</p>

<p>The <em>module load</em> command will introduce the environment necessary to run a particular program, in this case R version 3.5.1
The <em>srun</em> command will submit the job to the cluster.  The <em>srun</em> command has many parameter available, some of the most common are in this example and explained below</p>

<ul>
  <li>–pty - Required to run interactively</li>
  <li>–cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2</li>
  <li>–mem=2G - requests 2 GigaBytes (GB) of RAM.</li>
  <li>–time=2-00:00 - requests a runtime of up to 2 days (format is DAYS-HOURS:MINUTES), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching.  Keep in mind time is a resource along with CPU and Memory.</li>
  <li>–partition=main - requests a certain partition, in this case it requests the main partition, see the section on using cluster partitions for more information.</li>
  <li>R - the command you wish to run, this could also be matlab, mathematica, SAS, etc. (just remember to load the module first)# Parallel Jobs</li>
</ul>

<h1 id="parallel-processing">Parallel processing</h1>

<p>Running a job in parallel is a great way to utilize the power of the cluster.  So what is a parallel job/workflow?</p>

<ul>
  <li>Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this.</li>
  <li>Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores.</li>
  <li>Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs.</li>
  <li>GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL.</li>
</ul>

<p>It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations</p>

<h4 id="job-array-example">Job Array Example</h4>
<p>Here is an example of running a job array to run 50 simultaneous processes:</p>

<p><code class="highlighter-rouge">sbatch array.sh</code></p>

<p>The contents of the array.sh batch script looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#!/bin/bash</span>
  <span class="c">#SBATCH -a 1-50</span>
  <span class="c">#SBATCH --cpus-per-task=1</span>
  <span class="c">#SBATCH --mem-per-cpu=2G</span>
  <span class="c">#SBATCH --time=00:10:00</span>
  <span class="c">#SBATCH --partition=main</span>
  <span class="c">#SBATCH --mail-type=BEGIN,END,FAIL</span>
  <span class="c">#SBATCH --mail-user=me@email.com</span>

  bash addit.sh

</code></pre></div></div>

<p>So what do these parameter mean?:</p>

<ul>
  <li><em>-a</em> sets this up as a parallel array job (this sets up the “loop” that will be run</li>
  <li><em>–cpus-per-task</em> requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total</li>
  <li><em>–mem-per-cpu</em> request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM)</li>
  <li><em>–time</em> is the max run time for this job, 10 minutes in this case</li>
  <li><em>–partition</em> assigns this job to a partition</li>
  <li><em>bash addit.sh</em> run the shell script: addit.sh</li>
</ul>

<p>You will notice that this batch script runs the program addit.sh and passes the argument $SLURM_ARRAY_TASK_ID, this variable contains the array number, 1-50 in this case,</p>

<p>This is what the addit.sh script contains, it is a simple example but should show you how to run and use the job array and variables:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#!/bin/bash</span>
  <span class="nb">echo</span> <span class="s2">"The compute node this is running on is </span><span class="nv">$HOSTNAME</span><span class="s2">"</span>
  <span class="c"># $1 is the variable containing the Task ID which was passed to this script as an argument</span>
  <span class="c"># using the $SLURM_ARRAY_TASK_ID variable in the array.sh submit script</span>
  <span class="nb">echo</span> <span class="s2">"Task ID is </span><span class="nv">$1</span><span class="s2">"</span>
  <span class="c"># $RANDOM is a built-in pseudo random number generated number, we will add the Task ID to a random number</span>
  <span class="nv">sum</span><span class="o">=</span><span class="sb">`</span>expr <span class="nv">$1</span> + <span class="nv">$RANDOM</span><span class="sb">`</span>
  <span class="nb">echo</span> <span class="s2">"Output is </span><span class="nv">$sum</span><span class="s2">"</span>
</code></pre></div></div>
<div>
<p>Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs.
  <p> </div>

<h4 id="multi-threaded-or-multi-processing-job-example">Multi-threaded or Multi-processing Job Example</h4>

<p>Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka “task” in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the –cpus-per-task option.  Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 8GB of memory. The program itself is responsible for spawning the appropriate number of threads.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#!/bin/bash</span>
  <span class="c">#SBATCH --nodes=1</span>
  <span class="c">#SBATCH --ntasks=1</span>
  <span class="c">#SBATCH --cpus-per-task=12 # 12 threads per task</span>
  <span class="c">#SBATCH --time=02:00:00 # two hours</span>
  <span class="c">#SBATCH --mem=8G</span>
  <span class="c">#SBATCH --output=threaded.out</span>
  <span class="c">#SBATCH --job-name=threaded</span>
  <span class="c">#SBATCH --mail-type=BEGIN,END,FAIL</span>
  <span class="c">#SBATCH --mail-user=me@email.com</span>
  <span class="c"># Run multi-threaded application</span>
  module load java/1.8.0-91
  java <span class="nt">-jar</span> threaded-app.jar
</code></pre></div></div>

<h4 id="mpi-jobs">MPI Jobs</h4>

<p>Most users do not require MPI to run their jobs but many do.  Please read on if you want to learn more about using MPI for tightly-coupled jobs.</p>

<p>MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#!/bin/bash</span>
  <span class="c">#SBATCH --nodes=3</span>
  <span class="c">#SBATCH --tasks-per-node=8 # 8 MPI processes per node</span>
  <span class="c">#SBATCH --time=3-00:00:00</span>
  <span class="c">#SBATCH --mem=4G # 4 GB RAM per node</span>
  <span class="c">#SBATCH --output=mpi_job.log</span>
  <span class="c">#SBATCH -p main</span>
  <span class="c">#SBATCH --mail-type=BEGIN,END,FAIL</span>
  <span class="c">#SBATCH --mail-user=me@email.com</span>

  module load openmpi
  <span class="nb">echo</span> <span class="nv">$SLURM_JOB_NODELIST</span>
  mpirun <span class="nt">-np</span> 24 mpiscript.o

</code></pre></div></div>

<p>This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks.  I use this number to tell mpirun how many processes to start, -np 24</p>

<p>NOTE:  If using python or another language you will also need to add the –oversubscribe parameter to mpirun, eg.</p>

<p><code class="highlighter-rouge">mpirun --oversubscribe -np 24 mpiscript.py</code></p>

<p>More information about running MPI jobs within Slurm can be found here here: http://slurm.schedmd.com/mpi_guide.html.</p>

<h1 id="managing-jobs">Managing Jobs</h1>

<h4 id="cancelling-a-job">Cancelling a Job</h4>

<p>To cancel a job, first find the jobID, you can use the <em>vuw-myjobs</em> (or <em>squeue</em>) command to see a list of your jobs, including jobIDs.  Once you have that you can use the <em>scancel</em> command, eg</p>

<p><code class="highlighter-rouge">scancel 236789</code></p>

<p>To cancel all of your jobs you can use the -u flag followed by your username:</p>

<p><code class="highlighter-rouge">scancel -u harrelwe</code></p>

<h4 id="viewing-job-statistics">Viewing Job statistics</h4>

<p>If you want to get a quick view of all the jobs completed within the last 48 hours you can use the <em>vuw-history</em> command, for example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  vuw-job-history

  MY JOBS WITHIN LAST 48 HOURS
   JobID State MaxVMSize JobName End
  ------------ ---------- ---------- ---------- -------------------
  3054196 CANCELLED+ bash 2017-05-10T15:12:31
  3105606 CANCELLED+ bash 2017-05-11T14:21:43
  3105608 COMPLETED 268592K bash 2017-05-11T14:31:44
  3105622 COMPLETED 268592K bash 2017-05-11T14:34:17
  3105656 COMPLETED 268592K bash 2017-05-11T14:43:09
</code></pre></div></div>

<p>You can also get a report of your completed jobs using the <em>sacct</em> command.  For example if I wanted to get a report on how much memory my job used I could do the following:</p>

<p><code class="highlighter-rouge">sacct --format="MaxVMSize" -j 2156</code></p>

<ul>
  <li>MaxVMSize will report the maximum virtual memory (RAM plus swap space) used by my job in KiloBytes, divide that number by 1024 for MegaBytes, and then that number can be divided by 1024 to see it in GigaBytes..</li>
  <li>-j 2156 shows the information for job ID 2156</li>
  <li>type <em>man sacct</em> at a prompt in engaging to see the documentation on the <em>sacct</em> command</li>
</ul>

<h4 id="viewing-jobs-in-the-queue">Viewing jobs in the Queue</h4>

<p>To view your running jobs you can type the vuw-myjobs  eg:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>           [harrelwe@node142 ~]$ vuw-myjobs
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           7921967 main          bash harrelwe  R       0:12      1 node142
</code></pre></div></div>

<p>As you can see I have a single job running on node142 on the main partition</p>

<p>You can see all the jobs in the queues by running the <em>vuw-alljobs</em> command.  This will produce a very long list of jobs if the cluster is busy.</p>

<h4 id="job-queuing-aka-why-isnt-my-job-running">Job Queuing (aka Why isn’t my job running?)</h4>

<p>When a partition is busy, jobs will be placed in a queue.  You can observe this in the vuw-myjobs and vuw-alljobs commands.  The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing).</p>

<p>The resource manager will list a reason the job is pending, these reasons can include:</p>

<ul>
  <li><strong>Priority</strong> - Your job priority has been reduced to allow other users access to the cluster.  If no other user with normal priority is also pending then your job will start once resources are available.  Possible reasons why your priority has been lowered can include:  the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested.  The Slurm manager uses fair-share queuing to ensure the best use of the cluster.  You can google fair-share queuing  if you want to know more</li>
  <li><strong>Resources</strong>- There are insufficient resources to start your job.  Some combination of CPU, Memory, Time or other specialized resource are unavailable.  Once resources are freed up your job will begin to run.<br />
Time:   If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words:  it will never run).  Your time request must be less than or equal to the Partition Max Run-Time.  Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job.  You can see Max Run-Time for our partitions described in this document.  CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions.</li>
  <li><strong>QOSMaxCPUPerUser</strong> - This is a Quality of Service configuration to limit the number of CPUs per user.   The QOSMax is the maximum that can be requested for any single job.  If a user requests more CPUs than the QOSMax for a single job then the job will not run.  If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete.</li>
  <li><strong>QOSMaxMemPerUser</strong> - This is a Quality of Service configuration to limit the memory usage per user.   The QOSMax is the maximum that can be requested for any single job.  If a user requests more Memory than the QOSMax for a single job then the job will not run.  If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete.</li>
  <li><strong>PartitionTimeLimit</strong> - This means you have requested more time than the maximum runtime of the partition.  This document contains information about the different partitions, including max run-time.  Typing <em>vuw-partition</em> will also show the max run-time for the partitions available to you.</li>
  <li><strong>ReqNodeNotAvail</strong> - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs.  For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week.  To request time you can use the –time parameter.  Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition.</li>
  <li><strong>Required node not available (down, drained or reserved)</strong> - This is related to ReqNodeNotAvail, see above.</li>
</ul>

<h1 id="using-containers">Using Containers</h1>

<p>Researchers can use Docker or Singularity containers within the cluster.  This is a great way to run difficult-to-compile applications or to share workflows among colleagues.</p>

<h4 id="running-an-interactive-container">Running an interactive container</h4>

<p>User can run within a container interactively, this is great for testing code before running a job.  Here is an example of running within a docker container that has the blockchain software called BlockSci:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load singularity
srun --pty -c 4 --mem=16G bash
singularity pull docker://tislaamo/blocksci
singularity shell blocksci.simg
</code></pre></div></div>

<p>Once you have typed the <em>singularity shell</em> command you will be within the
container and can type the commands available from within the container such as
the BlockSci utility <strong>blocksci_parser</strong></p>

<h4 id="unning-a-container-in-batchr">Running a container in batch</h4>

<p>Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary.  Here is an example batch submit script that will run the BlockSci software that was created in an Ubuntu docker image, lets name the submit file runContainer.sh:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#SBATCH -J blockchain_parser
#SBATCH -c 4
#SBATCH --mem=16G
#SBATCH --mailtype=BEGIN,END,FAIL
#SBATCH --mail-user=myemail@email.net
#SBATCH --time=12:00:00

module load singularity
singularity exec blocksci.simg blocksci_parser --output-directory bitcoin-data update disk --coin-directory bitcoin
</code></pre></div></div>

<p>Now to run the file you can:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load singularity
singularity pull docker://tislaamo/blocksci
sbatch runContainer.sh
</code></pre></div></div>

<p>Note that <em>singularity shell</em> is primarily for interactive use and <em>singularity exec</em> (or possibly <em>singularity run</em>) are for executing the applications that were built within the container directly.  It is important to know how the container was created to make effective use of the software.</p>

<h1 id="examples">Examples</h1>

<h4 id="simple-python-program-using-virtualenv-and-pip">Simple Python program using virtualenv and pip</h4>

<p>First we need to create a working directory and move there</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir python_test
<span class="nb">cd </span>python_test
</code></pre></div></div>
<p>Next we load the python 3 module and use python 3 to create a python virtualenv.  This way we can install pip packages which are not installed on the cluster</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load python/3.6.6 
python3 <span class="nt">-m</span> venv mytest
</code></pre></div></div>

<p>Activate the <code class="highlighter-rouge">mytest</code> virtualenv and use pip to install the <code class="highlighter-rouge">webcolors</code> package</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>mytest/bin/activate
pip install webcolors
</code></pre></div></div>

<p>Create the file test.py with the following contents using nano</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">webcolors</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">socket</span> <span class="kn">import</span> <span class="n">gethostname</span>

<span class="n">colour_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">webcolors</span><span class="o">.</span><span class="n">css3_hex_to_names</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="n">requested_colour</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">colour_list</span><span class="p">))</span>
<span class="n">colour_name</span> <span class="o">=</span> <span class="n">colour_list</span><span class="p">[</span><span class="n">requested_colour</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Random colour name:"</span><span class="p">,</span> <span class="n">colour_name</span><span class="p">,</span> <span class="s">" on host: "</span><span class="p">,</span> <span class="n">gethostname</span><span class="p">())</span>
<span class="sb">``</span><span class="err">`</span><span class="n">bash</span>
<span class="n">Alternatively</span> <span class="n">download</span> <span class="n">it</span> <span class="k">with</span> <span class="n">wget</span>
<span class="sb">``</span><span class="err">`</span><span class="n">bash</span>
<span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">eResearchSandpit</span><span class="o">/</span><span class="n">vuwrc</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">python_venv</span><span class="o">/</span><span class="n">test</span><span class="o">.</span><span class="n">py</span>
</code></pre></div></div>

<p>Using nano create the submissions script called python_submit.sh with the following content - change <code class="highlighter-rouge">me@email.com</code> to your email address.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#</span>
<span class="c">#SBATCH --job-name=python_test</span>
<span class="c">#SBATCH -o python_test.out</span>
<span class="c">#SBATCH -e python_test.err</span>
<span class="c">#</span>
<span class="c">#SBATCH --cpus-per-task=1</span>
<span class="c">#SBATCH --mem-per-cpu=1G</span>
<span class="c">#SBATCH --time=10:00</span>
<span class="c">#</span>
<span class="c">#SBATCH --mail-type=BEGIN,END,FAIL</span>
<span class="c">#SBATCH --mail-user=me@email.com</span>

module load python/3.6.6 

<span class="nb">source </span>mytest/bin/activate 
srun python test.py
</code></pre></div></div>
<p>Alternatively download it with wget</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://raw.githubusercontent.com/eResearchSandpit/vuwrc/master/examples/python_venv/python_submit.sh
</code></pre></div></div>

<p>To submit your job to the Slurm scheduler</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch python_submit.sh
</code></pre></div></div>

<p>Check for your job on the queue with <code class="highlighter-rouge">squeue</code> though it might finish very fast.  The output files will appear in your working directory.</p>
</div>
   
   

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">CAD Research Cluster Documentation maintained by <a href="https://github.com/matty-plumski">matty-plumski</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a><p>
       
      </footer>
    </div>

       </section>




<!-- If 'fabricator=false' is present or attribute is not present at all in the annotation. -->

<!-- Default fallback for footer when not specified in the template. -->
<footer role="contentinfo">

    <!-- Links and contacts that can vary between sub-sites -->
    <section class="footer-secondary two-columns">
        <div class="centraliser">

            <!-- Sub-site's related or recommended links -->
            <div class="block">

                <h2>Useful links</h2>
                <nav role="navigation">
                    <ul role="menubar">
                        <li>
                            <a href="https://software-carpentry.org/" role="menuitem" >Software Carpentry</a>
                        </li>
                        <li>
                            <a href="https://hpc-carpentry.github.io/" role="menuitem">HPC Carpentry</a>
                        </li>
                        <li>
                            <a href="#" role="menuitem" title="Read PhD guidelines">PhD guidelines</a>
                        </li>
                        <li>
                            <a href="#" role="menuitem" title="Read Contacts and directories">Contacts and directories</a>
                        </li>
                        
                    </ul>
                </nav>

            </div>

            <!-- Sub-site's related links -->
            <div class="block contacts">

                <a href="#" title="See more contact information" class="link-more">Other contacts</a>
                <h2>Useful contacts</h2>

                <address>

                    <!-- Mail addresses -->
                    <ul class="mails">
                        <li>
                            <a href="mailto:compute@victoria.ac.nz" title="Send an email to compute@victoria.ac.nz">compute@victoria.ac.nz</a>
                        </li>
                    </ul>
                </address>

            </div>

        </div>
    </section>

    <!-- Related links from global site -->
    <section class="footer-secondary">
        <div class="centraliser">
            <div class="block">
                <h2>Victoria University</h2>

                <nav role="navigation">
                    <ul role="menubar">
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Faculties</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Contacts and directories</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Campuses</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Study</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Students</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Students</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Staff</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Careers</a></li>
                        <li role="menuitem"><a href="#" role="menuitem" title="Read about ...">Alumni</a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </section>

    <!-- Meta-links and contacts that are shared between sub-sites -->
    <section class="footer-primary">
        <div class="centraliser">

            <div class="block">

                <!-- Smaller version of logotype -->
                <div class="logo">
                    <a href="/" title="Victoria University of Wellington homepage">
                        <img class="logo"  src="images/logo-white-full.svg" alt="Victoria University of Wellington - Te Whare Wānanga o te Ūpoko o te Ika a Māui">
                    </a>
                </div>

                <!-- Global contacts -->
                <ul class="contacts">
                    <li>
                        <a title="Call Victoria about general enquiries" href="tel:+6444721000">+64 4 472 1000</a>
                    </li>
                    <li>
                        <a title="Write Victoria an email" href="mailto:info@victoria.ac.nz">info@victoria.ac.nz</a>
                    </li>
                    <li>
                        Enrolments: <a title="Call Victoria regarding enrolments" href="tel:+64800842867">0800 VICTORIA</a>
                    </li>
                    <li>
                        Emergency: <a title="Call in case of emergency" href="tel:+6444639999">+64 4 463 9999</a> or ext. 8888 (internal)
                    </li>
                </ul>

            </div>

        </div>
    </section>

</footer>


<!-- List STYLES -->


  <!-- jQuery -->
  <script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

<script src="https://static.victoria.ac.nz/toolkit.min.js"></script>


</body>
</html>
